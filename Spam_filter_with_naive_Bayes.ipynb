{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lovely spam - Building a naive Bayes spam filter\n",
    "\n",
    "`category: intermediate probability, machine learning classification`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/spam.jpg\" width=750 height=680 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and project's goal\n",
    "\n",
    "In this project, we're going to **build a spam filter using a multinomial** [naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).<br/>\n",
    "Spam is nowadays practically eveywhere: emails, phone sms, internet forums, socials. These unrequested messages are at best annoying, but may also constitute a threat (e.g. sending malicious links).\n",
    "\n",
    "Here, we're going to focus on a **spam filter for sms**. In our intentions, the filter would analyse the words that constitutes the message and tell us \"probably it's spam\", \"probably it's not spam\" or \"I'm uncertain\".\n",
    "For that, we'll use a dataset that you can download at the famous [UCI repository for Machine Learning](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). The authors built a collection of 5572 spam and non-spam (**ham** from now on) messages taken from different sources.\n",
    "\n",
    "The project's goal is to reach **at least 80% accuracy** in the classification (more on accuracy later).\n",
    "\n",
    "## What is a Naive Bayes classifier?\n",
    "\n",
    "Naive Bayes (nB) classifiers are a class of simple algorithms which allow to compute the probability for one set of features (e.g. words) to belong to a particular class. This is possible by means of the **Bayes' theorem**. They classify a given set of features by selecting the class with the highest associated probability.<br/>\n",
    "In their basic form, they are fast to train because their computational cost varies linearly with the number of features, and at run-time they require to only look up a table of already computed probabilities and perform some multiplications.\n",
    "\n",
    "In our case, we want to calculate the probability for one particular message containing the $n$ words $\\{w_{1}, w_{2}, ... w_{n}\\}$ to be a spam or a ham message. In symbols (and using the Bayes' theorem):\n",
    "\n",
    "$$P(spam|\\{w_{1}, w_{2}, ... w_{n}\\}) \\propto P(spam)\\cdot P(\\{w_{1}, w_{2}, ... w_{n}\\}|spam)$$ \n",
    "\n",
    "$$P(ham|\\{w_{1}, w_{2}, ... w_{n}\\}) \\propto P(ham)\\cdot P(\\{w_{1}, w_{2}, ... w_{n}\\}|ham)$$\n",
    "\n",
    "Now for the naiveness: in order to simplify the calculations, we make the assumption of **conditional independence**, i.e. we assume that the probability of each word to be in a spam/ham message is not influenced by the presence of the others. This way, the $P(\\{w_{1}, w_{2}, ... w_{n}\\}|spam)$ splits into the product of $n$ terms, like:\n",
    "\n",
    "$$P(spam|\\{w_{1}, w_{2}, ... w_{n}\\}) \\propto P(spam) \\prod_{i=1}^{n}P(w_{i}|spam)$$\n",
    "\n",
    "$$P(ham|\\{w_{1}, w_{2}, ... w_{n}\\}) \\propto P(ham) \\prod_{i=1}^{n}P(w_{i}|ham)$$\n",
    "\n",
    "$P(spam)$ and $P(ham)$ are simply the fraction of spam and ham messages in the dataset.<br/> \n",
    "We're going to calculate the probabilities for the i-th word as:\n",
    "\n",
    "$$P(w_{i}|spam) = \\frac{N_{w_{i}|spam} + \\alpha}{N_{spam} + \\alpha N_{voc}}$$\n",
    "\n",
    "$$P(w_{i}|ham) = \\frac{N_{w_{i}|ham} + \\alpha}{N_{ham} + \\alpha N_{voc}}$$\n",
    "\n",
    "where:\n",
    "- $N_{w_{i}|spam}$ is the number of times the word $w_{i}$ appears in the spam messages, counting each istance separately;\n",
    "- $N_{spam}$ is the number of words in all spam messages, again counting repeated words as separate contributions;\n",
    "- $N_{voc}$ is the number of unique words which appear in spam and ham words, a set which we'll call the  **vocabulary**;\n",
    "- $\\alpha$ is a parameter used for additive smoothing (i.e. avoiding that a probability for a word be zero if it doesn't appear in the vocabulary).\n",
    "\n",
    "The remaining terms have similar meaning.\n",
    "\n",
    "\n",
    "Two important observations to end our theoretical introduction:\n",
    "- the conditional independence assumption is a very strong one and it's not always guaranteed that it holds. Nonetheless, the nB classifier often gives good results even when this assumption formally fails;\n",
    "- the proportionality symbol $\\propto$ means that the formulas don't give the actual probability, but a quantity proportional to it. This is fine, because we're interested in **comparing** probabilities, not calculating their actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of results\n",
    "\n",
    "- The accuracy of the model on the test set is **98.7%**, much higher than our 80% initial goal.\n",
    "- Making the model more complicated by including sensitivity to letter case is of no help.\n",
    "- We found that spammers indeed use techniques to try and deceive the spam filters, and that probably explains the 14 misclassified sms (out of 1114). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to know the dataset\n",
    "\n",
    "Let's load the dataset into a Pandas dataframe called `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o.s. independent path to the dataset\n",
    "path_to_dataset = os.path.join(os.curdir, 'datasets', 'sms_spam_collection', 'dataset') \n",
    "\n",
    "data = pd.read_csv(path_to_dataset, \n",
    "                   sep='\\t',\n",
    "                   header=None,\n",
    "                   names=['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dataset is very simple: each row corresponds to a sms with its **label** (either spam or ham)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Label   5572 non-null   object\n",
      " 1   SMS     5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n",
      "\n",
      "\n",
      "The dataset contains 5572 sms and no null values.\n"
     ]
    }
   ],
   "source": [
    "data.info()\n",
    "print('\\n')\n",
    "print(f'The dataset contains {data.shape[0]} sms and no null values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we plot the relative frequency of spam and ham messages in the dataset.<br/> \n",
    "We see that most of the sms are legitimate messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# data for the plot\n",
    "label_counts = data['Label'].value_counts(normalize=True) * 100\n",
    "# drawing the barplot\n",
    "label_counts.plot.barh(ax=ax, color=['g', 'r'], alpha=0.6)\n",
    "\n",
    "# customize the plot\n",
    "ax.invert_yaxis()\n",
    "ax.set_title('spam and ham in the dataset', fontsize=20, weight='bold')\n",
    "ax.text(14, 1, str(round(label_counts['spam'], 1)) + '%', fontsize=17)\n",
    "ax.text(88, 0, str(round(label_counts['ham'], 1)) + '%', fontsize=17)\n",
    "ax.set_xlim(0,100)\n",
    "ax.tick_params(axis='x', bottom=False, labelbottom=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to split the data in a **training** set on which the parameters of the model will be calculated and a **test** set on which we'll measure how well the model behaves on data it has never seen.<br/>\n",
    "The proportions will be 80% for the training set and 20% for the test set, i.e.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 4458 messages\n",
      "Test:\t  1114 messages\n"
     ]
    }
   ],
   "source": [
    "print('Training:', round(data.shape[0] * 0.8), 'messages')\n",
    "print('Test:\\t ', round(data.shape[0] * 0.2), 'messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset to guarantee spam and ham are spread uniformly\n",
    "shuffled = data.sample(frac=1, random_state=1)\n",
    "\n",
    "# create the training and test sets\n",
    "train = shuffled.iloc[:4458].reset_index(drop=True).copy()\n",
    "test = shuffled.iloc[4458:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that spam and ham have the same proportions of the original dataset both in `train` and `test` dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.54105\n",
       "spam    13.45895\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Label'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.804309\n",
       "spam    13.195691\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Label'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, pretty much! We can start the data cleaning/wrangling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can actually calculate the parameters of the model, we must clean and wrangle the data a bit so that their format is appropriate. These are the first three messages in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yep, by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes, princess. Are you going to make me moan?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Welp apparently he retired</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                            SMS\n",
       "0   ham                   Yep, by the pretty sculpture\n",
       "1   ham  Yes, princess. Are you going to make me moan?\n",
       "2   ham                     Welp apparently he retired"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will define a function for removing the punctuation from a message. The function will also have a parameter `case_sensitive` through which we can control if the words in the message are lower-cased or not.<br/>\n",
    "Then we will split each sms into a list of its words.<br/>\n",
    "Finally, we will obtain the **vocabulary**, i.e. the list of all the unique words which appear in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sms_text(sms, case_sensitive=False):\n",
    "    '''Take one sms string and return the same string with\n",
    "       punctuation removed and (if case_sensitive==False) lowercased letters'''\n",
    "        \n",
    "    if not case_sensitive:        \n",
    "        return re.sub(r'\\W', ' ', sms).lower()\n",
    "    else:\n",
    "        return re.sub(r'\\W', ' ', sms)\n",
    "\n",
    "# apply the function to the SMS column    \n",
    "train['SMS'] = train['SMS'].apply(clean_sms_text)\n",
    "# obtain the list of words\n",
    "train['SMS'] = train['SMS'].str.split()\n",
    "\n",
    "# building the vocabulary\n",
    "vocabulary = []\n",
    "for words_list in train['SMS']:\n",
    "    for word in words_list:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary = list(set(vocabulary))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary has 7783 words.\n"
     ]
    }
   ],
   "source": [
    "print(f'The vocabulary has {len(vocabulary)} words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in the cleaning phase will be to count how many times each word in the dictionary appears in each message. In other terms, we want to obtain something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/final_df_naive_bayes.png\" width=500 height=350 />\n",
    "<!-- <p style=\"text-align: center;\">Pitcairn islanders in 1916</p> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dictionary\n",
    "word_counts_per_sms = {word: [0] * len(train['SMS']) for word in vocabulary}\n",
    "\n",
    "# count the occurrence of each word for each sms\n",
    "for i, sms in enumerate(train['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][i] += 1\n",
    "        \n",
    "# transform into a dataframe and concatenate with the training set\n",
    "counts_df = pd.DataFrame(word_counts_per_sms)\n",
    "train = pd.concat([train, counts_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>mustprovide</th>\n",
       "      <th>82050</th>\n",
       "      <th>fulfil</th>\n",
       "      <th>8p</th>\n",
       "      <th>slots</th>\n",
       "      <th>08718727868</th>\n",
       "      <th>formatting</th>\n",
       "      <th>quarter</th>\n",
       "      <th>...</th>\n",
       "      <th>yours</th>\n",
       "      <th>urgnt</th>\n",
       "      <th>langport</th>\n",
       "      <th>blankets</th>\n",
       "      <th>poortiyagi</th>\n",
       "      <th>worry</th>\n",
       "      <th>jus</th>\n",
       "      <th>freely</th>\n",
       "      <th>123</th>\n",
       "      <th>meaningful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 7785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  mustprovide  \\\n",
       "0   ham                  [yep, by, the, pretty, sculpture]            0   \n",
       "1   ham  [yes, princess, are, you, going, to, make, me,...            0   \n",
       "2   ham                    [welp, apparently, he, retired]            0   \n",
       "\n",
       "   82050  fulfil  8p  slots  08718727868  formatting  quarter  ...  yours  \\\n",
       "0      0       0   0      0            0           0        0  ...      0   \n",
       "1      0       0   0      0            0           0        0  ...      0   \n",
       "2      0       0   0      0            0           0        0  ...      0   \n",
       "\n",
       "   urgnt  langport  blankets  poortiyagi  worry  jus  freely  123  meaningful  \n",
       "0      0         0         0           0      0    0       0    0           0  \n",
       "1      0         0         0           0      0    0       0    0           0  \n",
       "2      0         0         0           0      0    0       0    0           0  \n",
       "\n",
       "[3 rows x 7785 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to calculate the model parameters. We'll start from the costants (refer to the initial paragraph `What is a Naive Bayes classifier?` for reviewing their meaning).\n",
    "\n",
    "They are: $P(spam)$, $P(ham)$, $N_{spam}$, $N_{ham}$, $N_{voc}$. The parameter $\\alpha$ will be set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_constant_parameters(train_dataset):\n",
    "    '''Return a tuple with all the constant parameters of the nB model'''\n",
    "    \n",
    "    # P(Spam) and P(Ham)\n",
    "    p_spam = train_dataset['Label'].value_counts(normalize=True)['spam']\n",
    "    p_ham = train_dataset['Label'].value_counts(normalize=True)['ham']\n",
    "\n",
    "    # number of words in spam, ham messages and unique words\n",
    "    n_spam = train_dataset[train_dataset['Label'] == 'spam']['SMS'].apply(len).sum()\n",
    "    n_ham = train_dataset[train_dataset['Label'] == 'ham']['SMS'].apply(len).sum()\n",
    "    n_vocabulary = len(vocabulary)\n",
    "\n",
    "    # Laplace smoothing\n",
    "    alpha = 1\n",
    "    \n",
    "    return (p_spam, p_ham, n_spam, n_ham, n_vocabulary, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_spam, p_ham, n_spam, n_ham, n_vocabulary, alpha = model_constant_parameters(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to calculate the probability $P(w_{i})$ for each word, both in ham and spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_variable_parameters(train_dataset):\n",
    "    '''Return two dictionaries with the probabilities of each word\n",
    "       being spam or ham'''\n",
    "    \n",
    "    # initialize the two dictionaries\n",
    "    prob_given_spam = {word: 0 for word in vocabulary}\n",
    "    prob_given_ham = {word:0 for word in vocabulary}\n",
    "\n",
    "    # spam messages probabilities\n",
    "    train_dataset_spam = train_dataset[train_dataset['Label'] == 'spam'].iloc[:, 2:]\n",
    "    for word in train_dataset_spam.columns:\n",
    "        prob_given_spam[word] = (train_dataset_spam[word].sum() + alpha) / (n_spam + alpha * n_vocabulary)\n",
    "\n",
    "    # ham messages probabilities\n",
    "    train_dataset_ham = train_dataset[train_dataset['Label'] == 'ham'].iloc[:, 2:]\n",
    "    for word in train_dataset_ham.columns:\n",
    "        prob_given_ham[word] = (train_dataset_ham[word].sum() + alpha) / (n_ham + alpha * n_vocabulary) \n",
    "        \n",
    "    return (prob_given_spam, prob_given_ham)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_given_spam, prob_given_ham = model_variable_parameters(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying new messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the parameters have been calculated, we are ready to apply the model to new messages.<br/>\n",
    "The function defined in the next cell will clean the message before classifying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message: str, case_sensitive=False) -> str:\n",
    "    '''Take a new message, perform some cleaning and calculate the \n",
    "       probabilities it's spam or ham.\n",
    "       Return \"spam\", \"ham\" or \"uncertain\" as classification.'''\n",
    "    \n",
    "    if not case_sensitive:\n",
    "        cleaned_msg = re.sub(r'\\W', ' ', message).lower()\n",
    "    else:\n",
    "        cleaned_msg = re.sub(r'\\W', ' ', message)\n",
    "    msg_words = cleaned_msg.split()\n",
    "    \n",
    "    # calculate the probabilities with the model\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    for word in msg_words:\n",
    "        p_spam_given_message *= prob_given_spam.get(word, 1)\n",
    "        p_ham_given_message *= prob_given_ham.get(word, 1)\n",
    "        \n",
    "    # classification\n",
    "    if p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    elif p_spam_given_message < p_ham_given_message:\n",
    "        return 'ham'\n",
    "    else:\n",
    "        return 'uncertain'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly try the function on two messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('WINNER!! This is the secret code to unlock the money: C3421.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"Sounds good, Tom, then see u there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the accuracy on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now verify that the model does a good job on messages it has never seen.<br/> \n",
    "For this, we will apply the classification algorithm to each message of the test set (1114 sms).<br/>\n",
    "\n",
    "The **accuracy** of the model is simply measured as:\n",
    "\n",
    "$$Accuracy = \\frac{no.\\:of\\:correctly\\:classified\\:messages}{no.\\:of\\:classified\\:messages}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has an accuracy of 98.7% over the test set.\n"
     ]
    }
   ],
   "source": [
    "test['predicted_label'] = test['SMS'].apply(classify)\n",
    "\n",
    "correct = (test['Label'] == test['predicted_label']).sum()            \n",
    "accuracy = correct / len(test)\n",
    "\n",
    "print(f'The model has an accuracy of {(accuracy * 100):.1f}% over the test set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reached a higher accuracy than our initial goal (80%): \n",
    "1100 messages were correctly classified out of 1114.\n",
    "\n",
    "Our spam filter definitely works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncorrectly classified messages: why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're curious to try and have an intuition for the reasons of misclassification. Let's take a look at those messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassified sms\n",
    "uncorrect_sms = test[test['Label'] != test['predicted_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not heard from U4 a while. Call me now am here all night with just my knickers on. Make me beg for it like U did last time 01223585236 XX Luv Nikiyu4.net\n",
      "--------------------\n",
      "More people are dogging in your area now. Call 09090204448 and join like minded guys. Why not arrange 1 yourself. There's 1 this evening. A£1.50 minAPN LS278BB\n",
      "--------------------\n",
      "Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50\n",
      "--------------------\n",
      "Hi babe its Chloe, how r u? I was smashed on saturday night, it was great! How was your weekend? U been missing me? SP visionsms.com Text stop to stop 150p/text\n",
      "--------------------\n",
      "0A$NETWORKS allow companies to bill for SMS, so they are responsible for their \"suppliers\", just as a shop has to give a guarantee on what they sell. B. G.\n",
      "--------------------\n",
      "RCT' THNQ Adrian for U text. Rgds Vatian\n",
      "--------------------\n",
      "2/2 146tf150p\n",
      "--------------------\n",
      "Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# print the spam messages classified as ham\n",
    "for i, row in uncorrect_sms[uncorrect_sms['predicted_label'] == 'ham'].iterrows():\n",
    "    print(row['SMS'])\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that these messages passed through the filter because most of them are long and mostly composed of common words. Furthermore, sometimes abbreviations and deliberate typos (like \"xafter\" instead of \"after\") are used, so that those words are simply skipped in the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlimited texts. Limited minutes.\n",
      "--------------------\n",
      "26th OF JULY\n",
      "--------------------\n",
      "Nokia phone is lovly..\n",
      "--------------------\n",
      "No calls..messages..missed calls\n",
      "--------------------\n",
      "We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# print the ham messages classified as spam\n",
    "for i, row in uncorrect_sms[uncorrect_sms['predicted_label'] == 'spam'].iterrows():\n",
    "    print(row['SMS'])\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, four of these messages are very short and thus just one or two words appearing in the spam class and not in the ham class are enough to jump to the wrong conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens if the model becomes case-sensitive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we will repeat all the previous steps on the training and test sets, but this time we will **not** convert all the letters to lower case, i.e. the model will be case-sensitive. We're curious to see if the accuracy changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the training and test sets\n",
    "shuffled = data.sample(frac=1, random_state=1)\n",
    "train = shuffled.iloc[:4458].reset_index(drop=True).copy()\n",
    "test = shuffled.iloc[4458:].reset_index(drop=True)\n",
    "\n",
    "# each sms gets splitted into a list of words\n",
    "train['SMS'] = train['SMS'].apply(clean_sms_text, case_sensitive=True)\n",
    "train['SMS'] = train['SMS'].str.split()\n",
    "\n",
    "# building the vocabulary\n",
    "vocabulary = []\n",
    "for words_list in train['SMS']:\n",
    "    for word in words_list:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dictionary\n",
    "word_counts_per_sms = {word: [0] * len(train['SMS']) for word in vocabulary}\n",
    "\n",
    "# count the occurrence of each word for each sms\n",
    "for i, sms in enumerate(train['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][i] += 1\n",
    "        \n",
    "# transform into a dataframe and concatenate with the training set\n",
    "counts_df = pd.DataFrame(word_counts_per_sms)\n",
    "train = pd.concat([train, counts_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has an accuracy of 97.7% over the test set.\n"
     ]
    }
   ],
   "source": [
    "# calculate the model parameters\n",
    "p_spam, p_ham, n_spam, n_ham, n_vocabulary, alpha = model_constant_parameters(train)\n",
    "prob_given_spam, prob_given_ham = model_variable_parameters(train)\n",
    "\n",
    "# classify the test-set sms \n",
    "test['predicted_label'] = test['SMS'].apply(classify, case_sensitive=True)\n",
    "\n",
    "# measure the accuracy\n",
    "correct = (test['Label'] == test['predicted_label']).sum()            \n",
    "accuracy = correct / len(test)\n",
    "print(f'The model has an accuracy of {(accuracy * 100):.1f}% over the test set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that making the model case-sensitive slightly worsens its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this project, we built a naive Bayes spam filter for sms and managed to correctly classify 98.7% of the sms in the test-set.<br/>\n",
    "We tried to explain in a qualitative way the reason for misclassification, which is probably unavoidable with our basic approach.<br/>\n",
    "Finally, we found that making the model case-senitive didn't improve its performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
